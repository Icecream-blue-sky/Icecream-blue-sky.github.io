<!DOCTYPE html>
<script src="/js/clicklove.js"></script>
<html  lang="zh">
<head>
    <meta charset="utf-8" />

<meta name="generator" content="Hexo 3.9.0" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>从零开始学习CNN系列——（二）一次简单的线性回归实践，初探深度学习的那些术语 - Icecream</title>


    <meta name="description" content="&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;从零开始学习CNN系列，是本人学习李宏毅老师的深度学习课、cs231n及吴恩达老师的深度学习课后总结的适合无基础小白入门深度学习的教程，会跟随本人的炼丹水平不断更改，如有错误，请多加指正。&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;此小节，我们将从一次简单的线性回归实践开始，解释平常听到的学习率(learning rate)、梯度下降(gradi">
<meta name="keywords" content="深度学习,CNN">
<meta property="og:type" content="article">
<meta property="og:title" content="从零开始学习CNN系列——（二）一次简单的线性回归实践，初探深度学习的那些术语">
<meta property="og:url" content="http://yoursite.com/2020/02/26/从零开始学习CNN系列——（二）一次简单的线性回归实践，初探深度学习的那些术语/index.html">
<meta property="og:site_name" content="Icecream">
<meta property="og:description" content="&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;从零开始学习CNN系列，是本人学习李宏毅老师的深度学习课、cs231n及吴恩达老师的深度学习课后总结的适合无基础小白入门深度学习的教程，会跟随本人的炼丹水平不断更改，如有错误，请多加指正。&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;此小节，我们将从一次简单的线性回归实践开始，解释平常听到的学习率(learning rate)、梯度下降(gradi">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/images/deep_learning/thumbnail.png">
<meta property="og:updated_time" content="2020-03-07T04:34:20.423Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="从零开始学习CNN系列——（二）一次简单的线性回归实践，初探深度学习的那些术语">
<meta name="twitter:description" content="&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;从零开始学习CNN系列，是本人学习李宏毅老师的深度学习课、cs231n及吴恩达老师的深度学习课后总结的适合无基础小白入门深度学习的教程，会跟随本人的炼丹水平不断更改，如有错误，请多加指正。&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;此小节，我们将从一次简单的线性回归实践开始，解释平常听到的学习率(learning rate)、梯度下降(gradi">
<meta name="twitter:image" content="http://yoursite.com/images/deep_learning/thumbnail.png">







<link rel="icon" href="/images/favicon.svg">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
<style>body>.footer,body>.navbar,body>.section{opacity:0}</style>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">

    
    
    
    
<link rel="stylesheet" href="/css/back-to-top.css">

    
    
    
    
    
    
    
    <link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
    
    <script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
    
    
        <script type="text/javascript" src="https://js.users.51.la/20469207.js"></script>
    

    


<link rel="stylesheet" href="/css/style.css">
</head>
<body class="is-3-column">
    <nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/logo3.jpg" alt="从零开始学习CNN系列——（二）一次简单的线性回归实践，初探深度学习的那些术语" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">首页</a>
                
                <a class="navbar-item"
                href="/archives">归档</a>
                
                <a class="navbar-item"
                href="/categories">分类</a>
                
                <a class="navbar-item"
                href="/tags">标签</a>
                
                <a class="navbar-item"
                href="/links">友链</a>
                
                <a class="navbar-item"
                href="/about">关于</a>
                
            </div>
            
            <div class="navbar-end">
                
                
                <a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;">
                    <i class="fas fa-list-ul"></i>
                </a>
                
                
                <a class="navbar-item search" title="搜索" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-9-widescreen has-order-2 column-main">
<div class="card">
    
    <div class="card-image">
        <span  class="image is-7by1">
            <img class="thumbnail" src="/images/deep_learning/thumbnail.png" alt="从零开始学习CNN系列——（二）一次简单的线性回归实践，初探深度学习的那些术语">
        </span>
    </div>
    
    <div class="card-content article ">
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                从零开始学习CNN系列——（二）一次简单的线性回归实践，初探深度学习的那些术语
            
        </h1>
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                
                <time class="level-item has-text-grey" datetime="2020-02-26T03:11:34.000Z"><i class="far fa-calendar-alt">&nbsp;</i>2020-02-26</time>
                
                <time class="level-item has-text-grey is-hidden-mobile" datetime="2020-03-07T04:34:20.423Z"><i class="far fa-calendar-check">&nbsp;</i>2020-03-07</time>
                
                
                <div class="level-item">
                <i class="far fa-folder-open has-text-grey"></i>&nbsp;
                <a class="has-link-grey -link" href="/categories/深度学习炼丹教程/">深度学习炼丹教程</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    <i class="far fa-clock"></i>&nbsp;
                    
                    
                    24 分钟 读完 (大约 3596 个字)
                </span>
                
                
                <span class="level-item has-text-grey" id="busuanzi_container_page_pv">
                    <i class="far fa-eye"></i>
                    <span id="busuanzi_value_page_pv">0</span>次访问
                </span>
                
            </div>
        </div>
        
        <div class="content">
            <p>&nbsp;&nbsp;&nbsp;&nbsp;从零开始学习CNN系列，是本人学习李宏毅老师的深度学习课、cs231n及吴恩达老师的深度学习课后总结的适合无基础小白入门深度学习的教程，会跟随本人的炼丹水平不断更改，如有错误，请多加指正。<br><br>&nbsp;&nbsp;&nbsp;&nbsp;此小节，我们将从一次简单的线性回归实践开始，解释平常听到的学习率(learning rate)、梯度下降(gradient decent)等机器学习术语的概念。<a id="more"></a></p>
<h2 id="1、使用机器学习求解经典的线性回归问题"><a href="#1、使用机器学习求解经典的线性回归问题" class="headerlink" title="1、使用机器学习求解经典的线性回归问题"></a>1、使用机器学习求解经典的线性回归问题</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;所谓经典的线性回归问题，就是寻找多个自变量和一个因变量之间的函数关系，即：<br>$$y=b+w \cdot x=b+\sum_{i=1}^n w_{i}*x_{i}$$<br>&nbsp;&nbsp;&nbsp;&nbsp;对于含n个自变量x和一个因变量y的问题，我们需要求出上式的参数b和向量w。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;是否还记得 系列（一） 中所说的利用机器学习解决问题的通用架构？我们现在即利用这个架构来解决这个线性回归问题。</p>
<ul>
<li><p><strong>step 1:建立模型。</strong><br>既然我们已经将其定义为线性回归问题，那么我们使用的函数模型就是上述的线性模型：<br>$$y=b+w \cdot x=b+\sum_{i=1}^n w_{i}*x_{i}$$</p>
</li>
<li><p><strong>step 2:针对每个函数(function)，利用训练集数据计算拟合度，即描述这个选定函数的好坏。</strong><br>首先设定$y_{p}$为模型预测值，$y$为真实值。暂且不管我们学过的任何误差函数，思考一下我们想要描述这函数的好坏最常规的思路是什么。那当然是直接将两者做差值处理：<br>$$L=\sum_{i=1}^{n}|y_{i}^{p}-y_{i}|$$<br>显然，$|y_{p}^{i}-y{i}|$越大，代表选定的函数越“坏”。这即是机器学习里常说的<strong>L1函数</strong>。不过最常用的误差函数还是之前所说的<strong>L2函数</strong>：<br>$$L =\sum_{i=1}^{n}\left(y_{i}-y_{i}^{p}\right)^{2}$$<br>这次例子我们选择用L2函数。所以：<br>$$L(w,b) = \sum_{i=1}^{n}\left(y_{i}-y_{i}^{p}\right)^{2}=\sum_{i=1}^{n}\left(y_{i}-(b+w \cdot x_{i}\right))^2$$<br>此处的$w$和$x_{i}$都是代表向量。</p>
</li>
<li><p><strong>step 3:寻找使误差函数最小的函数。</strong><br>$b$与$w$的可能性多种多样，即step 1函数模型下会有很多函数$f_{1}、f_{2}…$。最终选出的目标函数：<br>$$<br>f^{<em>}=\arg \min _{f} L(f)<br>$$<br>$$<br>w^{</em>}, b^{<em>}=\arg \min _{w, b} L(w, b)<br>$$<br>如何快速寻找出这个函数是解决本问题的关键。这就引出了本文最核心的内容——<strong>梯度下降法(Gradient Decent)</strong>。下面我用李老师的方法带领大家一探<strong>梯度下降法</strong>的奥秘！<br>首先我们尝试用其求出$w^{</em>}$,$w^{*}$是一个向量，其可能含有多个标量$w_{i}$，针对其中任意标量$w$，误差函数$L(w)$都是$w$的函数，假定其形状如图:</p>
<center>
<img src="/images/deep_learning/gradient_decent.png" width="50%" height="50%">
</center> 

<p>设$w$的初始值为$w_{0}$，即处于第一只猴子占领的位置，可以注意到，$L(w)$上的每一个点都代表一个函数，而 <strong>全局最优点(global minima)</strong> 位于最后一个红点，我们需要思考如何让猴子从当前位置，快速移动到<strong>全局最优点(global minima)</strong>。<br><strong>梯度下降法</strong>给出的思路是：计算当前点的斜率（即一维情况下的<strong>梯度</strong>）$\left.\frac{d L}{d w}\right|<em>{w=w</em>{0}}$，若其为负，即图中的情况，显然猴子应该往$w$增大的方向移动；若其为正，显然猴子应该往$w$减小的方向移动。这样才能更接近<strong>全局最优点(global minima)</strong>。现在我们确定了行进的方向，但具体要行进多远还没确定，这时候<strong>学习率(learning rate)</strong>就要出马了，它正是用来控制行进的幅度：<br>$$<br>-\left.\eta \frac{d L}{d w}\right|<em>{w=w</em>{0}}<br>$$<br>关于<strong>梯度下降法</strong>，cs231n中有个很形象的比喻——<strong>其好比蒙眼者徒步下山，每个点的梯度代表了地形的倾斜方向和程度，知道方向后，要跨出多长的步长呢？想尽快下山，则比如步幅大，但可能错过洼点；步幅小，虽然不容易错过洼点，但用时过长</strong>（通过学习率动态调节）。下面是大家最讨厌的公式推导环节（很基础的推导）：<br>因为$L(w,b)$实际上是多变量函数，所以严谨来说这里应该使用偏微分。对于任意$w_{i}$和b，初始值为$w_{i,0}$和$b_{0}$,学习率为$\eta$：<br>将前面的$L(w,b)$进一步展开<br>$$<br>L(w,b)=\sum_{j=1}^{n}\left(y_{j}-\left(b+\sum_{i=1}^{m} w_{i} * x_{j,i}\right)\right)^{2}<br>$$<br>$$<br>\frac{\partial L}{\partial w_{i}}=2\sum_{j=1}^{n}\left(y_{j}-\left(b+\sum_{i=1}^{m} w_{i} * x_{j,i}\right)\right)<em>(-x_{j,i})<br>$$<br>$$<br>\frac{\partial L}{\partial b}=2\sum_{j=1}^{n}\left(y_{j}-\left(b+\sum_{i=1}^{m} w_{i} * x_{j,i}\right)\right)</em>(-1)<br>$$<br>知道偏微分后，可以对$w_{i,0}$和$b_{0}$不断更新迭代：<br>$$<br>w_{i,1}=w_{i,0}-\left.\eta \frac{\partial L}{\partial w_{i}}\right|<em>{w=w</em>{i,0}, b=b_{0}}, b_{1}=b_{0}-\left.\eta \frac{\partial L}{\partial b}\right|<em>{w=w</em>{i,0}, b=b_{0}}<br>$$<br>$$<br>w_{i,2}=w_{i,1}-\left.\eta \frac{\partial L}{\partial w_{i}}\right|<em>{w=w</em>{i,1}, b=b_{1}}, b_{2}=b_{1}-\left.\eta \frac{\partial L}{\partial b}\right|<em>{w=w</em>{i,1}, b=b_{1}}<br>$$<br>$$<br>…<br>$$<br>如此不断迭代， 理论上当到达某点，使$\frac{\partial L}{\partial w_{i}}=0$和$\frac{\partial L}{\partial b}=0$时，停止迭代，这个点代表的函数即是我们寻找的<strong>误差最小的函数</strong>。</p>
</li>
</ul>
<h2 id="2、用代码实践上述理论推导"><a href="#2、用代码实践上述理论推导" class="headerlink" title="2、用代码实践上述理论推导"></a>2、用代码实践上述理论推导</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;以上是基于机器学习知识的理论推导。本小节我们用代码进行实践，以直观地观察<strong>梯度下降法（Gradient Descent）</strong> 解决线性回归问题的效果。</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">python3,gradient descent:</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line">x_data = [<span class="hljs-number">338.</span>,<span class="hljs-number">333.</span>,<span class="hljs-number">328.</span>,<span class="hljs-number">207.</span>,<span class="hljs-number">226.</span>,<span class="hljs-number">25.</span>,<span class="hljs-number">179.</span>,<span class="hljs-number">60.</span>,<span class="hljs-number">208.</span>,<span class="hljs-number">606.</span>]</span><br><span class="line">y_data = [<span class="hljs-number">640.</span>,<span class="hljs-number">633.</span>,<span class="hljs-number">619.</span>,<span class="hljs-number">393.</span>,<span class="hljs-number">428.</span>,<span class="hljs-number">27.</span>,<span class="hljs-number">193.</span>,<span class="hljs-number">66.</span>,<span class="hljs-number">226.</span>,<span class="hljs-number">1591.</span>]</span><br><span class="line"><span class="hljs-comment">#y_data = b + w*x_data</span></span><br><span class="line">x = np.arange(<span class="hljs-number">-200</span>,<span class="hljs-number">-100</span>,<span class="hljs-number">1</span>)<span class="hljs-comment">#bias的集合</span></span><br><span class="line">y = np.arange(<span class="hljs-number">-5</span>,<span class="hljs-number">5</span>,<span class="hljs-number">0.1</span>)<span class="hljs-comment">#weight的集合</span></span><br><span class="line">Z = np.zeros((len(x),len(y)))</span><br><span class="line">X,Y = np.meshgrid(x,y)</span><br><span class="line"><span class="hljs-comment">#bia与weight集合随意两个搭配，组成function set</span></span><br><span class="line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(x)):</span><br><span class="line">    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(len(y)):</span><br><span class="line">        b = x[i]</span><br><span class="line">        w = y[j]</span><br><span class="line">        Z[j][i] = <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(len(x_data)):</span><br><span class="line">            Z[j][i] = Z[j][i] + (y_data[n]-b-w*x_data[n])**<span class="hljs-number">2</span><span class="hljs-comment">#L(w,b)</span></span><br><span class="line">        Z[j][i] = Z[j][i]/len(x_data)<span class="hljs-comment">#实质上就是平均均方误差，L(w,b)/length</span></span><br><span class="line"><span class="hljs-comment">#梯度下降法起点(b0,w0)</span></span><br><span class="line">b = <span class="hljs-number">-120</span></span><br><span class="line">w = <span class="hljs-number">-4</span></span><br><span class="line">lr = <span class="hljs-number">0.0000001</span><span class="hljs-comment">#learning rate</span></span><br><span class="line">iteration = <span class="hljs-number">100000</span></span><br><span class="line">b_history = [b]</span><br><span class="line">w_history = [w]</span><br><span class="line"><span class="hljs-comment">#iterations，这里迭代一万次，用尽量多的次数来接近理论上的谷点</span></span><br><span class="line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(iteration):</span><br><span class="line">    b_grad = <span class="hljs-number">0.0</span></span><br><span class="line">    w_grad = <span class="hljs-number">0.0</span></span><br><span class="line">    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(len(x_data)):</span><br><span class="line">        b_grad = b_grad - <span class="hljs-number">2.0</span>*(y_data[n]-b-w*x_data[n])*<span class="hljs-number">1.0</span><span class="hljs-comment">#dL/db</span></span><br><span class="line">        w_grad = w_grad - <span class="hljs-number">2.0</span>*(y_data[n]-b-w*x_data[n])*x_data[n]<span class="hljs-comment">#dL/dw</span></span><br><span class="line">    <span class="hljs-comment">#(b,w)迭代</span></span><br><span class="line">    b = b - lr*b_grad</span><br><span class="line">    w = w - lr*w_grad</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment">#存储(b,w)的历史值，方便后面绘制轨迹</span></span><br><span class="line">    b_history.append(b)</span><br><span class="line">    w_history.append(w)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#plot the figure</span></span><br><span class="line">plt.contourf(x,y,Z,<span class="hljs-number">50</span>,alpha=<span class="hljs-number">0.5</span>,cmap=plt.get_cmap(<span class="hljs-string">'jet'</span>))<span class="hljs-comment">#等高线函数</span></span><br><span class="line">plt.plot([<span class="hljs-number">-188.4</span>],[<span class="hljs-number">2.67</span>],<span class="hljs-string">'x'</span>,ms=<span class="hljs-number">12</span>,markeredgewidth=<span class="hljs-number">3</span>,color=<span class="hljs-string">'orange'</span>)<span class="hljs-comment">#[-188.4][2.67]是最终找到的谷点</span></span><br><span class="line">plt.plot(b_history,w_history,<span class="hljs-string">'o-'</span>,ms=<span class="hljs-number">3</span>,lw=<span class="hljs-number">1.5</span>,color=<span class="hljs-string">'black'</span>)</span><br><span class="line">plt.xlim(<span class="hljs-number">-200</span>,<span class="hljs-number">-100</span>)</span><br><span class="line">plt.ylim(<span class="hljs-number">-5</span>,<span class="hljs-number">5</span>)</span><br><span class="line">plt.xlabel(<span class="hljs-string">r'$b$'</span>,fontsize=<span class="hljs-number">16</span>)</span><br><span class="line">plt.ylabel(<span class="hljs-string">r'$w$'</span>,fontsize=<span class="hljs-number">16</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>&nbsp;&nbsp;&nbsp;&nbsp;代码完全和李老师课上讲的一样，相信一定有读者对前面的数学公式推导手到擒来，，<strong>但看到代码就头皮发麻，不要慌！</strong>我对其中的关键点都做了详细的注释，另外再对其中的关键点解释一下：<br>&nbsp;&nbsp;&nbsp;&nbsp;首先问题背景依然是1小节提到的线性回归问题，只不过问题进一步简化，相当于<br>$$y=b+w \cdot x=b+\sum_{i=1}^n w_{i}*x_{i}$$<br>中$w$与$x$变成简单的一维向量)(n=1)，用李老师的例子来说就是：进化后的宝可梦的CP值(y)只受一个特征值影响(x)，比如只与生命值(hp)有关。<br>&nbsp;&nbsp;&nbsp;&nbsp;其次，代码中为了寻找函数集合(function set),将$b$和$w$各自限定在一定范围内,各取100个值：</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="hljs-number">-200</span>,<span class="hljs-number">-100</span>,<span class="hljs-number">1</span>)<span class="hljs-comment">#bias的集合</span></span><br><span class="line">y = np.arange(<span class="hljs-number">-5</span>,<span class="hljs-number">5</span>,<span class="hljs-number">0.1</span>)<span class="hljs-comment">#weight的集合</span></span><br></pre></td></tr></table></figure>

<p>然后将两个集合中的$b$和$w$任意组合，每个$(b,w)$组合都代表一个函数<br>$$y=b+w \cdot x$$<br>明白这两点，你会发现后续代码不过是依照我们前面理论推导的过程，一步步用代码翻译公式罢了。</p>
<center>
<img src="/images/deep_learning/gradient_decent_2.png" width="50%" height="50%">
</center> 

<center>
learning rate = 0.0000001结果
</center> 

<p><strong>ps:</strong> 这里的学习率(learning rate)很难调，你可以初步感受下炼丹调参的“乐趣”，当然后面有更高级的方法。</p>
<h2 id="3、梯度下降法-Gradient-Descent-的进一步思考——正则化-Regularization"><a href="#3、梯度下降法-Gradient-Descent-的进一步思考——正则化-Regularization" class="headerlink" title="3、梯度下降法(Gradient Descent)的进一步思考——正则化(Regularization)"></a>3、梯度下降法(Gradient Descent)的进一步思考——正则化(Regularization)</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;喜欢思考的读者心里应该会有一个疑问——你所有的推导都是基于确定的模型：<br>$$y=b+w \cdot x=b+\sum_{i=1}^n w_{i}*x_{i}$$<br>进行一系列理论推导，然而凭什么你就能确定理想的模型就只能是简单的线性模型？为什么不可以是下面的更复杂的函数？<br>$$y=b+w \cdot x=b+w_1 \cdot x + w_2 \cdot x^2$$<br>$$y=b+w \cdot x=b+w_1 \cdot x + w_2 \cdot x^2 + w_3 \cdot x^3$$<br>$$…$$<br>(注：式中的$x^2$不是常规意义上的向量的平方，而是例如$x=[x_1,x_2,x_3]$,$x^2=[x_1^2,x_2^2,x_3^2]$,这里只是为了便于书写，见谅）<br>&nbsp;&nbsp;&nbsp;&nbsp;针对这个问题，我们来逐步讨论。首先，如果将这些更复杂的函数模型代入2小节中的情形运算，结果会如何呢？<br>&nbsp;&nbsp;&nbsp;&nbsp;利用matlab的curve fitting tool工具箱对2中的数据进行多项式拟合，结果如下，依次为2、3、4、9阶多项式。</p>
<center>
<img src="/images/deep_learning/func_1.png" width="50%" height="50%">
</center>

<center>
<img src="/images/deep_learning/func_2.png" width="50%" height="50%">
</center> 

<center>
<img src="/images/deep_learning/func_3.png" width="50%" height="50%">
</center> 

<center>
<img src="/images/deep_learning/func_4.png" width="50%" height="50%">
</center> 

<center>
<img src="/images/deep_learning/func_9.png" width="50%" height="50%">
</center> 

<p>&nbsp;&nbsp;&nbsp;&nbsp;可以显而易见的发现，随着函数阶数的增加，模型拟合的误差越来越小，甚至到了9阶时，模型函数完全与$(x,y)$数据点重合，误差达到了0。这时候你也许会瞬间提出一个疑问：既然模型越复杂，误差越小，那我们每次都选择最复杂的模型训练不就可以了吗？<br>&nbsp;&nbsp;&nbsp;&nbsp;这个问题引发我们第一个讨论——<strong>是不是模型越复杂，训练结果就一定越好呢？</strong> 并不是。用来拟合的$(x,y)$数据我们通常称为<strong>训练集(training data)</strong> ，但是经过训练得出的最好的函数还需要在实际数据上检验，通常这部分数据我们称为<strong>测试集(testing data，请暂时忽略验证集，后续会讲解)</strong>，而核心问题是在训练集上误差很小的函数，在测试集上不一定很小，现实中，像9阶这种复杂模型函数，在测试集上误差可能会非常大。<br>&nbsp;&nbsp;&nbsp;&nbsp;进一步思考：为什么像9阶多项式这种复杂模型，在训练集上表现优异，而在测试集上往往表现很差？聪明的读者，<strong>肯定能发现是由于复杂模型的输出($y$)受输入($x$)影响较大</strong>，比如上述的9阶多项式中，因为存在$x^9$这一高阶项，导致输入($x$)发生轻微变化，输出($y$)就变化巨大，因此，就算测试集和训练集只有微小差异，最终在测试集的表现也远不如训练集，例如训练集中有(226,428)一点，而测试集中有其附近的点(227,430),训练出的模型函数导致就算x只增加了1，预测的y值也极具增加，在(227,430)产生极大误差。<br>&nbsp;&nbsp;&nbsp;&nbsp;简单的思考之后，我们可以发现，我们并不是讨厌所有复杂的模型，只是讨厌输出($y$)受输入($x$)影响较大的模型，换言之，我们喜欢更<strong>平滑(smooth)的模型</strong>，保证其在训练集上表现良好的同时，输入对输出的影响尽可能小。而<strong>正则化(Regularization)</strong>正是为了寻找平滑(smooth)模型而诞生的，其具体方法是通过更改$L(w,b)$的表达式：<br>$$L(w,b)=\sum_{j=1}^{n}\left(y_{j}-\left(b+\sum_{i=1}^{m} w_{i} * x_{j,i}\right)\right)^{2}+\lambda \sum_{i=1}^{m}w_i^2$$<br>看到这个你肯定会想问：<strong>为什么简单的加入$\lambda \sum_{i=1}^{m}w_i^2$(被称为正则项),就能使模型更平滑呢？</strong> 首先说句题外话，边提问，边思考，然后自己给出答案，是一种很不错的学习方式。其次，观察$L(w,b)$函数，要使$L(w,b)$尽可能小，$w_i$肯定要尽可能小，即 <strong>新的$L(w,b)$函数期望找到$w_i$和误差都尽可能小的模型</strong>。但$w_i$尽可能小，代表函数越平滑(smooth)吗?且看下式，我们探讨下训练出的模型中，$x_i$变化时$y_i$会发生什么样的变化。<br>$$y_p = b+\sum_{i=1}^{n} w_{i} * x_{i}$$<br>$$若 x_i = x_i + \Delta x_{i}$$<br>$$则 y_p = y_p + \sum_{i=1}^{n} w_{i} * \Delta x_{i}$$<br>显然，当$w_i$非常小时，即使$x_i$变化较大，预测值$y_p$也不会受很大影响，即正则化后，新的L(w,b)的确能找到更平滑的模型。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;这篇文章主要主要讲述了以下几点知识点：</p>
<ul>
<li>梯度下降法(Gradient Descent)</li>
<li>什么是常说的学习率(learning rate)</li>
<li>什么是正则化</li>
<li>L2损失函数(or误差函数)</li>
</ul>

        </div>
        
          <ul class="post-copyright">
          <li><strong>本文标题：</strong><a href="http://yoursite.com/2020/02/26/从零开始学习CNN系列——（二）一次简单的线性回归实践，初探深度学习的那些术语/">从零开始学习CNN系列——（二）一次简单的线性回归实践，初探深度学习的那些术语</a></li>
          <li><strong>本文作者：</strong><a href="http://yoursite.com">Icecream</a></li>
          <li><strong>本文链接：</strong><a href="http://yoursite.com/2020/02/26/从零开始学习CNN系列——（二）一次简单的线性回归实践，初探深度学习的那些术语/">http://yoursite.com/2020/02/26/从零开始学习CNN系列——（二）一次简单的线性回归实践，初探深度学习的那些术语/</a></li>
          <li><strong>发布时间：</strong>2020-02-26</li>
          <li><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！
          </li>
          </ul>
        
        
        <div class="level is-size-7 is-uppercase">
            <div class="level-start">
                <div class="level-item">
                    <span class="is-size-6 has-text-grey has-mr-7">#</span>
                    <a class="has-link-grey -link" href="/tags/CNN/">CNN</a>, <a class="has-link-grey -link" href="/tags/深度学习/">深度学习</a>
                </div>
            </div>
        </div>
        
        
        
        <div class="social-share"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css">
<script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3>
        <div class="buttons is-centered">
            
                
<a class="button is-info donate">
    <span class="icon is-small">
        <i class="fab fa-alipay"></i>
    </span>
    <span>支付宝</span>
    <div class="qrcode"><img src="/images/alipay.jpg" alt="支付宝"></div>
</a>

                
                
<a class="button is-success donate">
    <span class="icon is-small">
        <i class="fab fa-weixin"></i>
    </span>
    <span>微信</span>
    <div class="qrcode"><img src="/images/wechat.png" alt="微信"></div>
</a>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        <div class="level-start">
            <a class="level level-item has-link-grey  article-nav-prev" href="/2020/03/01/从零开始学习CNN系列——（三）梯度下降法的深度探索/">
                <i class="level-item fas fa-chevron-left"></i>
                <span class="level-item">从零开始学习CNN系列——（三）梯度下降法的深度探索</span>
            </a>
        </div>
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/2020/02/17/安利一些我特别喜欢的东西/">
                <span class="level-item">安利一些我特别喜欢的东西</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="title is-5 has-text-weight-normal">评论</h3>
        
<div id="valine-thread" class="content"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='//unpkg.com/valine/dist/Valine.min.js'></script>
<script>
    new Valine({
        el: '#valine-thread' ,
        notify: false,
        verify: false,
        app_id: 'xGYIJxb0wjPnfcBjmscobNQD-gzGzoHsz',
        app_key: 'x45tEyGlPiTsc8rPf8Inee1p',
        placeholder: '风过留痕，君过留言。(欢迎留下昵称和邮箱，以便收到回复通知)'
    });
</script>

    </div>
</div>
</div>
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered" style="flex-shrink: 1">
                <div>
                    
                    <figure class="image is-128x128 has-mb-6">
                        <img class="is-rounded" src="/images/header1.jpg" alt="Roger">
                    </figure>
                    
                    <p class="is-size-4 is-block">
                        Roger
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        好好照顾自己
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>西安</span>
                    </p>
                    
                </div>
            </div>
        </nav>
        <nav class="level is-mobile">
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        文章
                    </p>
                    <a href="/archives">
                        <p class="title has-text-weight-normal">
                            10
                        </p>
                    </a>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        分类
                    </p>
                    <a href="/categories">
                        <p class="title has-text-weight-normal">
                            4
                        </p>
                    </a>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        标签
                    </p>
                    <a href="/tags">
                        <p class="title has-text-weight-normal">
                            8
                        </p>
                    </a>
                </div>
            </div>
        </nav>
        
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://www.zhihu.com/people/zoutq/" target="_blank">
                关注我</a>
        </div>
        
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank"
                title="Github" href="https://github.com/Icecream-blue-sky">
                
                <i class="fab fa-github"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank"
                title="RSS" href="/">
                
                <i class="fas fa-rss"></i>
                
            </a>
            
        </div>
        
       <hr>
       <p id="evan">当我沿着一条路走下去的时候，心里总想着另一条路上的事。这种时候，我心里很乱。放声大哭从一个梦境进入另一个梦境，这是每个人都有的奢望。</p>
    </div>
</div>
    
        

    <div class="card widget" id="toc">
        <div class="card-content">
            <div class="menu">
                <h3 class="menu-label">
                    目录
                </h3>
                <ul class="menu-list"><li>
        <a class="is-flex" href="#1、使用机器学习求解经典的线性回归问题">
        <span class="has-mr-6">1</span>
        <span>1、使用机器学习求解经典的线性回归问题</span>
        </a></li><li>
        <a class="is-flex" href="#2、用代码实践上述理论推导">
        <span class="has-mr-6">2</span>
        <span>2、用代码实践上述理论推导</span>
        </a></li><li>
        <a class="is-flex" href="#3、梯度下降法-Gradient-Descent-的进一步思考——正则化-Regularization">
        <span class="has-mr-6">3</span>
        <span>3、梯度下降法(Gradient Descent)的进一步思考——正则化(Regularization)</span>
        </a></li><li>
        <a class="is-flex" href="#小结">
        <span class="has-mr-6">4</span>
        <span>小结</span>
        </a></li></ul>
            </div>
        </div>
    </div>

    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
        </div>
    
</div>

                
            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/logo3.jpg" alt="从零开始学习CNN系列——（二）一次简单的线性回归实践，初探深度学习的那些术语" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2020 Icecream&nbsp;
                <a href="https://www.beian.miit.gov.cn" target="_blank">陕ICP备19024510号-1</a>&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a>
                
                <br>
                <span class="busuanzi_container_site_uv">
                
                
                
                <span class="busuanzi_container_site_pv">
                本站总访问量-<span id="busuanzi_value_site_pv"></span>-次 | 您是第-<span id="busuanzi_value_site_uv"></span>-位小伙伴
                </span>
                </span>
                
                </p>
            </div>
            <div class="level-end">
            <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
            <script>
                var now = new Date(); 
                function createtime() { 
                    var grt= new Date("12/8/2019 18:32:00");//此处修改你的建站时间或者网站上线时间 
                    now.setTime(now.getTime()+250); 
                    days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
                    hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
                    if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
                    mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
                    seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
                    snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
                    document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "; 
                    document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
                } 
            setInterval("createtime()",250);
            </script>
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                        
                        <i class="fab fa-creative-commons"></i>&nbsp;<i class="fab fa-creative-commons-by"></i>&nbsp;<i class="fab fa-creative-commons-nc"></i>&nbsp;<i class="fab fa-creative-commons-sa"></i>&nbsp;
                        
                    </a>
                </p>
                
                </div>

            
            </div>
        </div>
    </div>
</footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("zh-CN");</script>

<script>
var IcarusThemeSettings = {
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


    <script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>



    
    
<script src="/js/animation.js"></script>

    
    
<script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
<script src="/js/gallery.js" defer></script>

    
    
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>

    
    <script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>
    
    
<a id="back-to-top" title="回到顶端" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>

    
    
    
    
    
    
    
    
    
    
    
    
    

    


<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="想要查找什么..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":200,"height":400},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>