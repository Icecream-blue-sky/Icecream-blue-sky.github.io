<!DOCTYPE html>
<script src="/js/clicklove.js"></script>
<html  lang="zh">
<head>
    <meta charset="utf-8" />

<meta name="generator" content="Hexo 3.9.0" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>从零开始学习CNN系列——（三）梯度下降法的深度探索 - Icecream</title>


    <meta name="description" content="&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;从零开始学习CNN系列，是本人学习李宏毅老师的深度学习课、cs231n及吴恩达老师的深度学习课后总结的适合无基础小白入门深度学习的教程，会跟随本人的炼丹水平不断更改，如有错误，请多加指正。&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;此小节，我们将对上一小节讲到的梯度下降法(Graident Descent)做进一步的探索，讲解常见的几种梯度下降法">
<meta name="keywords" content="深度学习,CNN">
<meta property="og:type" content="article">
<meta property="og:title" content="从零开始学习CNN系列——（三）梯度下降法的深度探索">
<meta property="og:url" content="http://yoursite.com/2020/03/01/从零开始学习CNN系列——（三）梯度下降法的深度探索/index.html">
<meta property="og:site_name" content="Icecream">
<meta property="og:description" content="&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;从零开始学习CNN系列，是本人学习李宏毅老师的深度学习课、cs231n及吴恩达老师的深度学习课后总结的适合无基础小白入门深度学习的教程，会跟随本人的炼丹水平不断更改，如有错误，请多加指正。&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;此小节，我们将对上一小节讲到的梯度下降法(Graident Descent)做进一步的探索，讲解常见的几种梯度下降法">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/images/deep_learning/thumbnail.png">
<meta property="og:updated_time" content="2020-06-11T10:17:48.442Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="从零开始学习CNN系列——（三）梯度下降法的深度探索">
<meta name="twitter:description" content="&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;从零开始学习CNN系列，是本人学习李宏毅老师的深度学习课、cs231n及吴恩达老师的深度学习课后总结的适合无基础小白入门深度学习的教程，会跟随本人的炼丹水平不断更改，如有错误，请多加指正。&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;此小节，我们将对上一小节讲到的梯度下降法(Graident Descent)做进一步的探索，讲解常见的几种梯度下降法">
<meta name="twitter:image" content="http://yoursite.com/images/deep_learning/thumbnail.png">







<link rel="icon" href="/images/favicon.svg">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
<style>body>.footer,body>.navbar,body>.section{opacity:0}</style>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">

    
    
    
    
<link rel="stylesheet" href="/css/back-to-top.css">

    
    
    
    
    
    
    
    <link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
    
    <script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
    
    
        <script type="text/javascript" src="https://js.users.51.la/20469207.js"></script>
    

    


<link rel="stylesheet" href="/css/style.css">
</head>
<body class="is-3-column">
    <nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/logo3.jpg" alt="从零开始学习CNN系列——（三）梯度下降法的深度探索" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">首页</a>
                
                <a class="navbar-item"
                href="/archives">归档</a>
                
                <a class="navbar-item"
                href="/categories">分类</a>
                
                <a class="navbar-item"
                href="/tags">标签</a>
                
                <a class="navbar-item"
                href="/links">友链</a>
                
                <a class="navbar-item"
                href="/about">关于</a>
                
            </div>
            
            <div class="navbar-end">
                
                
                <a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;">
                    <i class="fas fa-list-ul"></i>
                </a>
                
                
                <a class="navbar-item search" title="搜索" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-9-widescreen has-order-2 column-main">
<div class="card">
    
    <div class="card-image">
        <span  class="image is-7by1">
            <img class="thumbnail" src="/images/deep_learning/thumbnail.png" alt="从零开始学习CNN系列——（三）梯度下降法的深度探索">
        </span>
    </div>
    
    <div class="card-content article ">
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                从零开始学习CNN系列——（三）梯度下降法的深度探索
            
        </h1>
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                
                <time class="level-item has-text-grey" datetime="2020-03-01T10:29:10.000Z"><i class="far fa-calendar-alt">&nbsp;</i>2020-03-01</time>
                
                <time class="level-item has-text-grey is-hidden-mobile" datetime="2020-06-11T10:17:48.442Z"><i class="far fa-calendar-check">&nbsp;</i>2020-06-11</time>
                
                
                <div class="level-item">
                <i class="far fa-folder-open has-text-grey"></i>&nbsp;
                <a class="has-link-grey -link" href="/categories/深度学习炼丹教程/">深度学习炼丹教程</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    <i class="far fa-clock"></i>&nbsp;
                    
                    
                    33 分钟 读完 (大约 4911 个字)
                </span>
                
                
                <span class="level-item has-text-grey" id="busuanzi_container_page_pv">
                    <i class="far fa-eye"></i>
                    <span id="busuanzi_value_page_pv">0</span>次访问
                </span>
                
            </div>
        </div>
        
        <div class="content">
            <p>&nbsp;&nbsp;&nbsp;&nbsp;从零开始学习CNN系列，是本人学习李宏毅老师的深度学习课、cs231n及吴恩达老师的深度学习课后总结的适合无基础小白入门深度学习的教程，会跟随本人的炼丹水平不断更改，如有错误，请多加指正。<br><br>&nbsp;&nbsp;&nbsp;&nbsp;此小节，我们将对上一小节讲到的梯度下降法(Graident Descent)做进一步的探索，讲解常见的几种梯度下降法变种，并用代码加以实践。<a id="more"></a></p>
<h2 id="1、Adagrad"><a href="#1、Adagrad" class="headerlink" title="1、Adagrad"></a>1、Adagrad</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;此方法的核心思想是<strong>自适应动态调节学习率(learning rate)</strong>，运用同样思想的梯度下降法有很多种，Adagrad只是其中一种。再介绍Adagrad具体内容前，我们先来了解这个方法提出的问题背景。</p>
<center>
<img src="/images/deep_learning/ada_1.png" width="50%" height="50%">
</center> 

<p>&nbsp;&nbsp;&nbsp;&nbsp;对于Loss曲线如上图的情况，分别选择极大(very large)、较大(large)、合适(just make)、较小(small)的学习率，我们得到了左图的四种结果：</p>
<ul>
<li><strong>极大(very Large):</strong> 学习率过大，直接跳过目标点，损失(Loss)一直降不下去，达不到目标点。</li>
<li><strong>较大(large):</strong> 学习率较大，迭代一定次数后，损失(Loss)在两个点中循环跳跃，无论迭代多少次，最终都错过目标点。</li>
<li><strong>合适(just make):</strong> 学习率刚好合适，迭代一定次数后，刚好到达目标点。</li>
<li><strong>较小(small):</strong> 学习率较小，虽然能到达目标点，但是迭代时间过长。</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;以上即是上一节讲过的“原始“的梯度下降法存在的问题，它提醒我们要想达到较好的训练结果，学习率不能是常数，而应该是能动态改变的。如往常的文章一样，读者可以先自己思考：如果是你，会怎样解决这个问题？<br>&nbsp;&nbsp;&nbsp;&nbsp;我相信，大多数人能够想到的一种简单方案是——<strong>一开始，我们离目标点很远，应该步子迈大点，即选择较大的学习率；随着迭代次数增多，逐渐靠近目标点，应该放慢步伐，即选择较小的学习率，以防错过目标点。</strong> 这个方案的专业名称叫做<strong>Vanilla Gradient Descent</strong>,数学表达式为：<br>$$<br>w^{t+1} = w^{t}-\eta^{t} g^{t}<br>$$<br>$$<br>\eta^{t}=\frac{\eta}{\sqrt{t+1}}，g^{t}=\frac{\partial L\left(\theta^{t}\right)}{\partial w}<br>$$<br>$$<br>其中t为迭代次数，\eta^{t}为学习率，g^{t}为梯度<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;但这个方案的核心问题是对于每个参数，学习率的变化是一样的，而稍微思考一下，对于不同的参数，最优的学习率变化必定有差异，即<strong>最好的方案是对于不同的参数，我们能够选择不同的动态学习率</strong>。Adagrad就是基于这种考虑的一种方案，数学表达式为：<br>$$<br>w^{t+1} = w^{t}-\frac{\eta^{t}}{\sigma^{t}} g^{t}<br>$$<br>$$<br>\eta^{t}=\frac{\eta}{\sqrt{t+1}}，g^{t}=\frac{\partial L\left(\theta^{t}\right)}{\partial w}，\sigma^{t}=\sqrt{\frac{1}{t+1} \sum_{i=0}^{t}\left(g^{i}\right)^{2}}<br>$$<br>进一步化简可得：<br>$$<br>w^{t+1} \leftarrow w^{t}-\frac{\eta}{\sqrt{\sum_{i=0}^{t}\left(g^{i}\right)^{2}}} g^{t}<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;关于Adagrad的合理性的解释，个人不是很同意李老师的解释，感觉有点牵强。以现有的知识水平，个人认为Adagrad综合了：<strong>迭代次数越多，学习率需要逐渐降低</strong>和<strong>若前面的迭代中的平均梯度偏小（或偏大），学习率也应当较大（或较小）两种情况的考量</strong>。但从数学上解释，还不能完全理解，以后如果有更深的理解会回来更新。<br>&nbsp;&nbsp;&nbsp;&nbsp;炼丹可不能纸上谈兵，代码实践才是王道，我们将上篇的代码稍微做个更改以实现Adagrad:</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line">x_data = [<span class="hljs-number">338.</span>,<span class="hljs-number">333.</span>,<span class="hljs-number">328.</span>,<span class="hljs-number">207.</span>,<span class="hljs-number">226.</span>,<span class="hljs-number">25.</span>,<span class="hljs-number">179.</span>,<span class="hljs-number">60.</span>,<span class="hljs-number">208.</span>,<span class="hljs-number">606.</span>]</span><br><span class="line">y_data = [<span class="hljs-number">640.</span>,<span class="hljs-number">633.</span>,<span class="hljs-number">619.</span>,<span class="hljs-number">393.</span>,<span class="hljs-number">428.</span>,<span class="hljs-number">27.</span>,<span class="hljs-number">193.</span>,<span class="hljs-number">66.</span>,<span class="hljs-number">226.</span>,<span class="hljs-number">1591.</span>]</span><br><span class="line"><span class="hljs-comment">#y_data = b + w*x_data</span></span><br><span class="line">x = np.arange(<span class="hljs-number">-200</span>,<span class="hljs-number">-100</span>,<span class="hljs-number">1</span>)<span class="hljs-comment">#bias的集合</span></span><br><span class="line">y = np.arange(<span class="hljs-number">-5</span>,<span class="hljs-number">5</span>,<span class="hljs-number">0.1</span>)<span class="hljs-comment">#weight的集合</span></span><br><span class="line">Z = np.zeros((len(x),len(y)))</span><br><span class="line">X,Y = np.meshgrid(x,y)</span><br><span class="line"><span class="hljs-comment">#bia与weight集合随意两个搭配，组成function set</span></span><br><span class="line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(x)):</span><br><span class="line">    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(len(y)):</span><br><span class="line">        b = x[i]</span><br><span class="line">        w = y[j]</span><br><span class="line">        Z[j][i] = <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(len(x_data)):</span><br><span class="line">            Z[j][i] = Z[j][i] + (y_data[n]-b-w*x_data[n])**<span class="hljs-number">2</span><span class="hljs-comment">#L(w,b)</span></span><br><span class="line">        Z[j][i] = Z[j][i]/len(x_data)<span class="hljs-comment">#实质上就是平均均方误差，L(w,b)/length</span></span><br><span class="line"><span class="hljs-comment">#梯度下降法起点(b0,w0)</span></span><br><span class="line">b = <span class="hljs-number">-120</span></span><br><span class="line">w = <span class="hljs-number">-4</span></span><br><span class="line">lr = <span class="hljs-number">1</span></span><br><span class="line">b_lr = <span class="hljs-number">0</span><span class="hljs-comment">#learning rate</span></span><br><span class="line">w_lr = <span class="hljs-number">0</span></span><br><span class="line">iteration = <span class="hljs-number">100000</span></span><br><span class="line">b_history = [b]</span><br><span class="line">w_history = [w]</span><br><span class="line">b_sum_grad = <span class="hljs-number">0</span><span class="hljs-comment">#参数b的梯度的平方和</span></span><br><span class="line">w_sum_grad = <span class="hljs-number">0</span><span class="hljs-comment">#参数w的梯度的平方和</span></span><br><span class="line"><span class="hljs-comment">#iterations，这里迭代一万次，用尽量多的次数来接近理论上的谷点</span></span><br><span class="line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(iteration):</span><br><span class="line">    b_grad = <span class="hljs-number">0.0</span></span><br><span class="line">    w_grad = <span class="hljs-number">0.0</span></span><br><span class="line">    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(len(x_data)):</span><br><span class="line">        b_grad = b_grad - <span class="hljs-number">2.0</span>*(y_data[n]-b-w*x_data[n])*<span class="hljs-number">1.0</span><span class="hljs-comment">#dL/db</span></span><br><span class="line">        w_grad = w_grad - <span class="hljs-number">2.0</span>*(y_data[n]-b-w*x_data[n])*x_data[n]<span class="hljs-comment">#dL/dw</span></span><br><span class="line">    b_sum_grad = b_sum_grad + b_grad**<span class="hljs-number">2</span></span><br><span class="line">    w_sum_grad = w_sum_grad + w_grad**<span class="hljs-number">2</span></span><br><span class="line">    <span class="hljs-comment">#(b,w)迭代</span></span><br><span class="line">    b_lr = lr/np.sqrt(b_sum_grad)</span><br><span class="line">    w_lr = lr/np.sqrt(w_sum_grad)</span><br><span class="line">    b = b - b_lr*b_grad</span><br><span class="line">    w = w - w_lr*w_grad</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment">#存储(b,w)的历史值，方便后面绘制轨迹</span></span><br><span class="line">    b_history.append(b)</span><br><span class="line">    w_history.append(w)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#plot the figure</span></span><br><span class="line">plt.contourf(X,Y,Z,<span class="hljs-number">50</span>,alpha=<span class="hljs-number">0.5</span>,cmap=plt.get_cmap(<span class="hljs-string">'jet'</span>))<span class="hljs-comment">#等高线函数</span></span><br><span class="line">plt.plot([<span class="hljs-number">-188.4</span>],[<span class="hljs-number">2.67</span>],<span class="hljs-string">'x'</span>,ms=<span class="hljs-number">12</span>,markeredgewidth=<span class="hljs-number">3</span>,color=<span class="hljs-string">'orange'</span>)<span class="hljs-comment">#[-188.4][2.67]是最终找到的谷点</span></span><br><span class="line">plt.plot(b_history,w_history,<span class="hljs-string">'o-'</span>,ms=<span class="hljs-number">3</span>,lw=<span class="hljs-number">1.5</span>,color=<span class="hljs-string">'black'</span>)</span><br><span class="line">plt.xlim(<span class="hljs-number">-200</span>,<span class="hljs-number">-100</span>)</span><br><span class="line">plt.ylim(<span class="hljs-number">-5</span>,<span class="hljs-number">5</span>)</span><br><span class="line">plt.xlabel(<span class="hljs-string">r'$b$'</span>,fontsize=<span class="hljs-number">16</span>)</span><br><span class="line">plt.ylabel(<span class="hljs-string">r'$w$'</span>,fontsize=<span class="hljs-number">16</span>)</span><br><span class="line">plt.show()</span><br><span class="line">print(<span class="hljs-string">"over!"</span>)</span><br></pre></td></tr></table></figure>

<p>&nbsp;&nbsp;&nbsp;&nbsp;效果还是挺显著的，见下图。</p>
<center>
<img src="/images/deep_learning/ada_2.png" width="50%" height="50%">
</center> 

<h2 id="2、Stochastic-Gradient-Descent（随机梯度下降法）"><a href="#2、Stochastic-Gradient-Descent（随机梯度下降法）" class="headerlink" title="2、Stochastic Gradient Descent（随机梯度下降法）"></a>2、Stochastic Gradient Descent（随机梯度下降法）</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;对于随机梯度下降法，这里我们引用博主<a href="https://www.cnblogs.com/louyihang-loves-baiyan/p/5136447.html" target="_blank" rel="noopener">楼燚航</a>的理解：<br><br>&nbsp;&nbsp;&nbsp;&nbsp;相对于原始的随机梯度下降法，可以看到多了随机两个字，随机也就是说用<strong>样本中的一个例子来近似所有的样本</strong>，来调整θ，因而随机梯度下降是会带来一定的问题，因为计算得到的并不是准确的一个梯度，容易陷入到局部最优解中。<br><br>&nbsp;&nbsp;&nbsp;&nbsp;下图是<strong>原始梯度下降法</strong>和<strong>随机梯度下降法</strong>的公式对比：<br>$$<br>L=\sum_{n}\left(\hat{y}^{n}-\left(b+\sum w_{i} x_{i}^{n}\right)\right)^{2},\theta^{i}=\theta^{i-1}-\eta \nabla L\left(\theta^{i-1}\right)，原始梯度下降法<br>$$<br>$$<br>L^{n}=\left(\hat{y}^{n}-\left(b+\sum w_{i} x_{i}^{n}\right)\right)^{2},\theta^{i}=\theta^{i-1}-\eta \nabla L^{n}\left(\theta^{i-1}\right)，随机梯度下降法<br>$$<br>其中，$\nabla$是指梯度，$L^{n}$表达式中的$n$是随机的，意为从所有数据中随机抽出一组，利用这组数据的损失(loss)代表全局损失(loss)。<br>&nbsp;&nbsp;&nbsp;&nbsp;可以很清楚的看到，随机梯度下降法的确是利用所有样本中的一个例子来近似所有样本，相信也会有读者对其合理性产生质疑，我的理解是：随机梯度下降法是为了减少计算量，尝试将所有超参数向着局部损失减小的方向进行，而局部损失非常小的点<strong>可能</strong>恰是全局最优点——因为全局最优时，局部损失其实也就必定很小。<br>&nbsp;&nbsp;&nbsp;&nbsp;在实际应用中，随机梯度下降法因为迭代速度非常快，大受欢迎，最大的缺点就是最终收敛的解不一定是全局最优解。下面我们继续对上一节的代码进行更改，测试下随机梯度下降法的实际表现：</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> random</span><br><span class="line">x_data = [<span class="hljs-number">338.</span>,<span class="hljs-number">333.</span>,<span class="hljs-number">328.</span>,<span class="hljs-number">207.</span>,<span class="hljs-number">226.</span>,<span class="hljs-number">25.</span>,<span class="hljs-number">179.</span>,<span class="hljs-number">60.</span>,<span class="hljs-number">208.</span>,<span class="hljs-number">606.</span>]</span><br><span class="line">y_data = [<span class="hljs-number">640.</span>,<span class="hljs-number">633.</span>,<span class="hljs-number">619.</span>,<span class="hljs-number">393.</span>,<span class="hljs-number">428.</span>,<span class="hljs-number">27.</span>,<span class="hljs-number">193.</span>,<span class="hljs-number">66.</span>,<span class="hljs-number">226.</span>,<span class="hljs-number">1591.</span>]</span><br><span class="line"><span class="hljs-comment">#y_data = b + w*x_data</span></span><br><span class="line">x = np.arange(<span class="hljs-number">-200</span>,<span class="hljs-number">-100</span>,<span class="hljs-number">1</span>)<span class="hljs-comment">#bias的集合</span></span><br><span class="line">y = np.arange(<span class="hljs-number">-5</span>,<span class="hljs-number">5</span>,<span class="hljs-number">0.1</span>)<span class="hljs-comment">#weight的集合</span></span><br><span class="line">Z = np.zeros((len(x),len(y)))</span><br><span class="line">X,Y = np.meshgrid(x,y)</span><br><span class="line"><span class="hljs-comment">#bia与weight集合随意两个搭配，组成function set</span></span><br><span class="line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(x)):</span><br><span class="line">    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(len(y)):</span><br><span class="line">        b = x[i]</span><br><span class="line">        w = y[j]</span><br><span class="line">        Z[j][i] = <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(len(x_data)):</span><br><span class="line">            Z[j][i] = Z[j][i] + (y_data[n]-b-w*x_data[n])**<span class="hljs-number">2</span><span class="hljs-comment">#L(w,b)</span></span><br><span class="line">        Z[j][i] = Z[j][i]/len(x_data)<span class="hljs-comment">#实质上就是平均均方误差，L(w,b)/length</span></span><br><span class="line"><span class="hljs-comment">#梯度下降法起点(b0,w0)</span></span><br><span class="line">b = <span class="hljs-number">-120</span></span><br><span class="line">w = <span class="hljs-number">-4</span></span><br><span class="line">lr = <span class="hljs-number">0.000001</span></span><br><span class="line">iteration = <span class="hljs-number">0</span></span><br><span class="line">b_history = [b]</span><br><span class="line">w_history = [w]</span><br><span class="line">loss = <span class="hljs-number">10</span>**<span class="hljs-number">6</span><span class="hljs-comment">#总损失</span></span><br><span class="line">threshold = <span class="hljs-number">10</span>**<span class="hljs-number">5</span>+<span class="hljs-number">2000</span><span class="hljs-comment">#最大允许误差</span></span><br><span class="line">max_iters = <span class="hljs-number">1000000</span><span class="hljs-comment">#最大迭代次数</span></span><br><span class="line"><span class="hljs-comment">#iterations，这里迭代一万次，用尽量多的次数来接近理论上的谷点</span></span><br><span class="line"><span class="hljs-keyword">while</span>(loss &gt; threshold):</span><br><span class="line">    loss = <span class="hljs-number">0</span></span><br><span class="line">    b_grad = <span class="hljs-number">0.0</span></span><br><span class="line">    w_grad = <span class="hljs-number">0.0</span></span><br><span class="line">    n = random.randint(<span class="hljs-number">0</span>,<span class="hljs-number">9</span>)</span><br><span class="line">    b_grad = b_grad - <span class="hljs-number">2.0</span>*(y_data[n]-b-w*x_data[n])*<span class="hljs-number">1.0</span><span class="hljs-comment">#dL/db</span></span><br><span class="line">    w_grad = w_grad - <span class="hljs-number">2.0</span>*(y_data[n]-b-w*x_data[n])*x_data[n]<span class="hljs-comment">#dL/dw</span></span><br><span class="line">    b = b - lr*b_grad</span><br><span class="line">    w = w - lr*w_grad</span><br><span class="line">    <span class="hljs-comment">#存储(b,w)的历史值，方便后面绘制轨迹</span></span><br><span class="line">    b_history.append(b)</span><br><span class="line">    w_history.append(w)</span><br><span class="line">    <span class="hljs-comment">#计算全局损失，以判断是否应该终止迭代</span></span><br><span class="line">    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(len(x_data)):</span><br><span class="line">        y_pre = b + w*x_data[n]</span><br><span class="line">        loss =  loss + (y_data[n]-y_pre)**<span class="hljs-number">2</span></span><br><span class="line">    loss = loss/len(x_data)</span><br><span class="line">    iteration = iteration + <span class="hljs-number">1</span></span><br><span class="line">    <span class="hljs-keyword">if</span> iteration &gt; max_iters:</span><br><span class="line">        <span class="hljs-keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#plot the figure</span></span><br><span class="line">print(<span class="hljs-string">"iterations:%d"</span>%(iteration))</span><br><span class="line">plt.contourf(X,Y,Z,<span class="hljs-number">50</span>,alpha=<span class="hljs-number">0.5</span>,cmap=plt.get_cmap(<span class="hljs-string">'jet'</span>))<span class="hljs-comment">#等高线函数</span></span><br><span class="line">plt.plot([<span class="hljs-number">-188.4</span>],[<span class="hljs-number">2.67</span>],<span class="hljs-string">'x'</span>,ms=<span class="hljs-number">12</span>,markeredgewidth=<span class="hljs-number">3</span>,color=<span class="hljs-string">'orange'</span>)<span class="hljs-comment">#[-188.4][2.67]是最终找到的谷点</span></span><br><span class="line">plt.plot(b_history,w_history,<span class="hljs-string">'o-'</span>,ms=<span class="hljs-number">3</span>,lw=<span class="hljs-number">1.5</span>,color=<span class="hljs-string">'black'</span>)</span><br><span class="line">plt.xlim(<span class="hljs-number">-200</span>,<span class="hljs-number">-100</span>)</span><br><span class="line">plt.ylim(<span class="hljs-number">-5</span>,<span class="hljs-number">5</span>)</span><br><span class="line">plt.xlabel(<span class="hljs-string">r'$b$'</span>,fontsize=<span class="hljs-number">16</span>)</span><br><span class="line">plt.ylabel(<span class="hljs-string">r'$w$'</span>,fontsize=<span class="hljs-number">16</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>输出：iterations = 4861428</p>
<center>
<img src="/images/deep_learning/sgd_1.png" width="50%" height="50%">
</center> 

<p>&nbsp;&nbsp;&nbsp;&nbsp;因为阈值没有设置为和目标点误差完全一样，所以运行后只到达了目标点附近。从结果图，可以明显看出，随机梯度下降法并没有想象中那么给力，还是有一些缺陷，比如<strong>迭代过程中波动严重</strong>，这是因为每次迭代时，算法不一定按照全局最优方向进行，所以间接导致迭代次数增加。不过，运算速度确实快，迭代了4861428次，总时间也没比Adagrad的100000次少多少。</p>
<h2 id="3、Momentum（动量法）"><a href="#3、Momentum（动量法）" class="headerlink" title="3、Momentum（动量法）"></a>3、Momentum（动量法）</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;Momentum在中文里被译为动量，即高中物理所学的那个动量。Momentum梯度下降法的核心思想也正是基于动量（或者说惯性）的原理而来的，个人找到的对该方法原理最直观的解释如下（链接：<a href="https://blog.csdn.net/tsyccnh/article/details/76270707）：" target="_blank" rel="noopener">https://blog.csdn.net/tsyccnh/article/details/76270707）：</a></p>
<blockquote>
<p>如果把梯度下降法想象成一个小球从山坡到山谷的过程，那么一般梯度下降法的小球是这样移动的：从A点开始，计算当前A点的坡度，沿着坡度最大的方向走一段路，停下到B。在B点再看一看周围坡度最大的地方，沿着这个坡度方向走一段路，再停下。确切的来说，这并不像一个球，更像是一个正在下山的盲人，每走一步都要停下来，用拐杖来来探探四周的路，再走一步停下来，周而复始，直到走到山谷。而<strong>一个真正的小球要比这聪明多了，从A点滚动到B点的时候，小球带有一定的初速度，在当前初速度下继续加速下降，小球会越滚越快，更快的奔向谷底。</strong> momentum 动量法就是模拟这一过程来加速神经网络的优化的。</p>
</blockquote>
<center>
<img src="/images/deep_learning/momentum_1.jpg" width="50%" height="50%">
</center>

<p>&nbsp;&nbsp;&nbsp;&nbsp;让我们从最简单的情况开始——只有一个参数w需要学习，即上图所示的情况。此时的Loss曲线是简单的曲线，Loss关于w的梯度方向是沿x方向的一维向量，只能向前或者向后：<br>$$<br>\nabla L = \frac{\partial L}{\partial w}<br>$$<br>一般的梯度下降法会按如下方式移动：<br>$$<br>w^t = w^{t-1} - \eta^{t-1}\nabla L(w^{t-1})<br>$$<br>而Momentum梯度下降法则会考虑小球从上一个位置移动到当前位置残留的动量（这里的动量实际上就是上一个位置的负梯度向量，也即小球剩余的水平速度）:<br>$$<br>\begin{array}{l}<br>v^{t}=\mu v^{t-1}-\eta^{t-1} \nabla L\left(w^{t-1}\right) \<br>w^{t}=w^{t-1}+v^{t} \<br>其中v^t代表第t次更新参数积累的动量，也即第t次参数更新的方向; \<br>v^0 = 0, 即起始位置的动量为0; \<br>0 \le \mu \le 1是人工可调节的常数，一般为0.9; \<br>t \ge 1 ;\<br>\end{array}<br>$$<br>可能一眼看上去不好理解，我们可以一步步来：<br>$$<br>\begin{array}{l}<br>v^{1}=\mu v^{0}-\eta^{0} \nabla L\left(w^{0}\right) = -\eta^{0} \nabla L\left(w^{0}\right) \<br>w^{1}=w^{0}+v^{1} \<br>v^{2}=\mu v^{1}-\eta^{1} \nabla L\left(w^{1}\right) \<br>w^{2}=w^{1}+v^{2} \<br>…<br>\end{array}<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;结合上图进行理解，t = 1时，小球从起始点开始移动到第二个位置，由于是起始点，小球并无初速度，因此第一次移动积累的动量为0，迭代方式和普通梯度下降法一致；t=2时，小球从第二个位置开始移动到第三个位置，这时观察$v^2$ 可以发现多了个$\mu v^1$ ，从物理学角度，这可以理解为动量或者说惯性的作用，因为小球从起始点移动到第二个位置时还保留着水平速度，在进行第二次移动时，Momentum梯度下降法考虑了这个速度，将其和当前位置的梯度下降方向进行结合，以此来作为小球最终的行进方向。这点正和本节开头阐述的思想一致。<br>&nbsp;&nbsp;&nbsp;&nbsp;其实抛却“动量”这个对非物理专业可能比较难懂的概念，Momentum梯度下降法中的动量$v^t$可以做如下的简化理解：</p>
<blockquote>
<p>$v^t$可以简单理解为，在进行第t次参数更新的时候，将第t-1次参数更新的方向与第t次计算得到的负梯度向量进行矢量叠加（原本的参数更新方向），并作为第t次参数更新的实际方向。</p>
</blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;上面小球的例子只考虑了一个参数的简单情况，对于多个参数，原理也同理，因为无论有多少个参数，梯度都是空间中的向量，其表达方式不会变，这里引用<a href="：https://blog.csdn.net/tsyccnh/article/details/76270707">博客</a>中一张非常直观的图来进行理解。</p>
<center>
<img src="/images/deep_learning/momentum_2.jpg" width="70%" height="70%">
</center>
<center>
对于本文，图中的梯度下降方向应该理解为负梯度向量方向，<br>
momentum的公式不止一种
</center>

<p>&nbsp;&nbsp;&nbsp;&nbsp;相对于一般的梯度下降法，Momentum梯度下降法最显著的优点就是不容易卡在局部最优点，因为当处于局部最优点的时候，由于“动量”的作用，“小球”会冲出局部最优点。实际运用中，其一般是结合SGD一起使用的。<br>&nbsp;&nbsp;&nbsp;&nbsp;最后当然是代码实践环节,参考了下<a href="https://github.com/tsycnh/mlbasic" target="_blank" rel="noopener">代码</a>，也是来源于上面的<a href="https://blog.csdn.net/tsyccnh/article/details/76270707" target="_blank" rel="noopener">博客</a>。代码如下：</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> random</span><br><span class="line"><span class="hljs-keyword">from</span> mpl_toolkits.mplot3d <span class="hljs-keyword">import</span> Axes3D</span><br><span class="line">x_data = [<span class="hljs-number">338.</span>,<span class="hljs-number">333.</span>,<span class="hljs-number">328.</span>,<span class="hljs-number">207.</span>,<span class="hljs-number">226.</span>,<span class="hljs-number">25.</span>,<span class="hljs-number">179.</span>,<span class="hljs-number">60.</span>,<span class="hljs-number">208.</span>,<span class="hljs-number">606.</span>]</span><br><span class="line">y_data = [<span class="hljs-number">640.</span>,<span class="hljs-number">633.</span>,<span class="hljs-number">619.</span>,<span class="hljs-number">393.</span>,<span class="hljs-number">428.</span>,<span class="hljs-number">27.</span>,<span class="hljs-number">193.</span>,<span class="hljs-number">66.</span>,<span class="hljs-number">226.</span>,<span class="hljs-number">1591.</span>]</span><br><span class="line"><span class="hljs-comment">#y_data = b + w*x_data</span></span><br><span class="line">x = np.arange(<span class="hljs-number">-200</span>,<span class="hljs-number">-100</span>,<span class="hljs-number">1</span>)<span class="hljs-comment">#bias的集合</span></span><br><span class="line">y = np.arange(<span class="hljs-number">-5</span>,<span class="hljs-number">5</span>,<span class="hljs-number">0.1</span>)<span class="hljs-comment">#weight的集合</span></span><br><span class="line">Z = np.zeros((len(x),len(y)))</span><br><span class="line">X,Y = np.meshgrid(x,y)</span><br><span class="line"><span class="hljs-comment">#bia与weight集合随意两个搭配，组成function set</span></span><br><span class="line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(x)):</span><br><span class="line">    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(len(y)):</span><br><span class="line">        b = x[i]</span><br><span class="line">        w = y[j]</span><br><span class="line">        <span class="hljs-comment">#matplotlib绘制3D图时都是默认按列优先的顺序进行绘制，一般都要进行转置</span></span><br><span class="line">        <span class="hljs-comment">#这里用Z[j][i]其实就是提前转置</span></span><br><span class="line">        Z[j][i] = <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(len(x_data)):</span><br><span class="line">            Z[j][i] = Z[j][i] + (y_data[n]-b-w*x_data[n])**<span class="hljs-number">2</span><span class="hljs-comment">#L(w,b)</span></span><br><span class="line">        Z[j][i] = Z[j][i]/len(x_data)<span class="hljs-comment">#实质上就是平均均方误差，L(w,b)/length</span></span><br><span class="line"><span class="hljs-comment">#梯度下降法起点(b0,w0)</span></span><br><span class="line">b = <span class="hljs-number">-120</span></span><br><span class="line">w = <span class="hljs-number">-4</span></span><br><span class="line">lr = <span class="hljs-number">0.0000001</span></span><br><span class="line">b_history = [b]</span><br><span class="line">w_history = [w]</span><br><span class="line">loss = <span class="hljs-number">10</span>**<span class="hljs-number">6</span><span class="hljs-comment">#总损失</span></span><br><span class="line">threshold = <span class="hljs-number">10</span>**<span class="hljs-number">5</span>+<span class="hljs-number">2000</span><span class="hljs-comment">#最大允许误差</span></span><br><span class="line">max_iters = <span class="hljs-number">10000</span><span class="hljs-comment">#最大迭代次数</span></span><br><span class="line">gamma = <span class="hljs-number">0.7</span></span><br><span class="line"><span class="hljs-comment">#动量</span></span><br><span class="line">vb = <span class="hljs-number">0</span></span><br><span class="line">vw = <span class="hljs-number">0</span></span><br><span class="line"><span class="hljs-comment">#plot the figure</span></span><br><span class="line">fig = plt.figure(<span class="hljs-number">1</span>)</span><br><span class="line">fig.suptitle(<span class="hljs-string">'learning rate: %.6f method:momentum'</span>%(lr))</span><br><span class="line"><span class="hljs-comment">#绘制三维曲面图</span></span><br><span class="line">ax = fig.add_subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,projection=<span class="hljs-string">'3d'</span>)</span><br><span class="line">ax.plot_surface(X,Y,Z,cmap=<span class="hljs-string">'rainbow'</span>)</span><br><span class="line"><span class="hljs-comment">#绘制等高线</span></span><br><span class="line">plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)</span><br><span class="line"><span class="hljs-comment">#填充等高线曲面</span></span><br><span class="line">plt.contourf(X,Y,Z,<span class="hljs-number">50</span>,alpha=<span class="hljs-number">0.5</span>,cmap=plt.get_cmap(<span class="hljs-string">'jet'</span>))<span class="hljs-comment">#等高线函数</span></span><br><span class="line">plt.xlim(<span class="hljs-number">-200</span>,<span class="hljs-number">-100</span>)</span><br><span class="line">plt.ylim(<span class="hljs-number">-5</span>,<span class="hljs-number">5</span>)</span><br><span class="line">plt.xlabel(<span class="hljs-string">r'$b$'</span>,fontsize=<span class="hljs-number">16</span>)</span><br><span class="line">plt.ylabel(<span class="hljs-string">r'$w$'</span>,fontsize=<span class="hljs-number">16</span>)</span><br><span class="line">plt.plot([<span class="hljs-number">-188.4</span>],[<span class="hljs-number">2.67</span>],<span class="hljs-string">'x'</span>,ms=<span class="hljs-number">12</span>,markeredgewidth=<span class="hljs-number">3</span>,color=<span class="hljs-string">'orange'</span>)<span class="hljs-comment">#[-188.4][2.67]是最终找到的谷点</span></span><br><span class="line"></span><br><span class="line">plt.ion()</span><br><span class="line"><span class="hljs-comment">#iterations，这里迭代一万次，用尽量多的次数来接近理论上的谷点</span></span><br><span class="line"><span class="hljs-keyword">for</span> iteration <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,max_iters+<span class="hljs-number">1</span>):</span><br><span class="line">    loss = <span class="hljs-number">0</span></span><br><span class="line">    b_grad = <span class="hljs-number">0.0</span></span><br><span class="line">    w_grad = <span class="hljs-number">0.0</span></span><br><span class="line">    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,len(x_data)):</span><br><span class="line">        b_grad = b_grad - <span class="hljs-number">2.0</span>*(y_data[n]-b-w*x_data[n])*<span class="hljs-number">1.0</span><span class="hljs-comment">#dL/db</span></span><br><span class="line">        w_grad = w_grad - <span class="hljs-number">2.0</span>*(y_data[n]-b-w*x_data[n])*x_data[n]<span class="hljs-comment">#dL/dw</span></span><br><span class="line">    vb = gamma*vb - lr*b_grad</span><br><span class="line">    vw = gamma*vw - lr*w_grad</span><br><span class="line">    b = b + vb</span><br><span class="line">    w = w + vw</span><br><span class="line">    <span class="hljs-comment">#存储(b,w)的历史值，方便后面绘制轨迹</span></span><br><span class="line">    b_history.append(b)</span><br><span class="line">    w_history.append(w)</span><br><span class="line">    <span class="hljs-comment">#计算全局损失，以判断是否应该终止迭代</span></span><br><span class="line">    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(len(x_data)):</span><br><span class="line">        y_pre = b + w*x_data[n]</span><br><span class="line">        loss =  loss + (y_data[n]-y_pre)**<span class="hljs-number">2</span></span><br><span class="line">    loss = loss/len(x_data)<span class="hljs-comment">#前面Z[j][i]也除了len(x_data)，做统一处理</span></span><br><span class="line">    <span class="hljs-comment">#绘制三维图面图上的动态轨迹</span></span><br><span class="line">    ax.scatter(b,w,loss,s=<span class="hljs-number">10</span>,color=<span class="hljs-string">'black'</span>)</span><br><span class="line">    <span class="hljs-comment">#绘制等高线图上的动态轨迹</span></span><br><span class="line">    plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)</span><br><span class="line">    plt.scatter(b,w,s=<span class="hljs-number">5</span>,color=<span class="hljs-string">'black'</span>)</span><br><span class="line">    plt.plot(b_history,w_history,<span class="hljs-string">'o-'</span>,ms=<span class="hljs-number">3</span>,lw=<span class="hljs-number">1.5</span>,color=<span class="hljs-string">'black'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    plt.pause(<span class="hljs-number">0.01</span>)</span><br><span class="line">print(<span class="hljs-string">"iterations:%d, loss:%d"</span>%(iteration,loss))</span><br><span class="line">plt.show()</span><br><span class="line">plt.pause(<span class="hljs-number">999999999</span>)</span><br></pre></td></tr></table></figure>

<p>&nbsp;&nbsp;&nbsp;&nbsp;很不幸，由于没有对数据做归一化，最终“小球”就是不断在谷底来回”徘徊“，始终到不了终点。但确实可以非常直观地看到，当learning rate和$\mu$设置得稍微大一些，“小球”直接冲出谷底，绝不回头，不信可以试试，代码运行结果是动态的！</p>
<center>
<img src="/images/deep_learning/momentum_3.png" width="100%" height="100%">
</center>

<center>
learing rate = 0.0000001, 动量因子为0.7
</center>


<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;本小节主要讲了两种梯度下降法的优化算法，当然还有Adam、NAG、Momentum等其它优化算法，以后用到的时候会滚回来更新。</p>
<ul>
<li><strong>Adagrad</strong>算法通过针对不同的参数，动态选择不同的学习率，来优化原始梯度下降法。</li>
<li><strong>Stochastic Gradient Descent（随机梯度下降法）</strong> 算法通过随机选取一组样本中的一个样本，来近似代表所有样本，利用其损失函数梯度代替全局损失函数梯度来更新超参数，以获得更快的学习速度，但容易陷入局部最优且迭代过程波动严重。</li>
<li><strong>Momentum</strong> 算法通过模拟现实世界中物理运动的动量（惯性）作用，在每次更新参数时，将上一次参数更新的方向与当前点的负梯度下降方向进行矢量相加，作为最终的参数更新方向。由于“动量”的作用，该算法可以有效解决一般算法容易陷入“局部最优点”的缺点。</li>
</ul>

        </div>
        
          <ul class="post-copyright">
          <li><strong>本文标题：</strong><a href="http://yoursite.com/2020/03/01/从零开始学习CNN系列——（三）梯度下降法的深度探索/">从零开始学习CNN系列——（三）梯度下降法的深度探索</a></li>
          <li><strong>本文作者：</strong><a href="http://yoursite.com">Icecream</a></li>
          <li><strong>本文链接：</strong><a href="http://yoursite.com/2020/03/01/从零开始学习CNN系列——（三）梯度下降法的深度探索/">http://yoursite.com/2020/03/01/从零开始学习CNN系列——（三）梯度下降法的深度探索/</a></li>
          <li><strong>发布时间：</strong>2020-03-01</li>
          <li><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！
          </li>
          </ul>
        
        
        <div class="level is-size-7 is-uppercase">
            <div class="level-start">
                <div class="level-item">
                    <span class="is-size-6 has-text-grey has-mr-7">#</span>
                    <a class="has-link-grey -link" href="/tags/CNN/">CNN</a>, <a class="has-link-grey -link" href="/tags/深度学习/">深度学习</a>
                </div>
            </div>
        </div>
        
        
        
        <div class="social-share"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css">
<script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3>
        <div class="buttons is-centered">
            
                
<a class="button is-info donate">
    <span class="icon is-small">
        <i class="fab fa-alipay"></i>
    </span>
    <span>支付宝</span>
    <div class="qrcode"><img src="/images/alipay.jpg" alt="支付宝"></div>
</a>

                
                
<a class="button is-success donate">
    <span class="icon is-small">
        <i class="fab fa-weixin"></i>
    </span>
    <span>微信</span>
    <div class="qrcode"><img src="/images/wechat.png" alt="微信"></div>
</a>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        <div class="level-start">
            <a class="level level-item has-link-grey  article-nav-prev" href="/2020/03/08/排名不好如何保研外校（中科大保研经验分享）/">
                <i class="level-item fas fa-chevron-left"></i>
                <span class="level-item">排名不好如何保研外校（中科大保研经验分享）</span>
            </a>
        </div>
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/2020/02/26/从零开始学习CNN系列——（二）一次简单的线性回归实践，初探深度学习的那些术语/">
                <span class="level-item">从零开始学习CNN系列——（二）一次简单的线性回归实践，初探深度学习的那些术语</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="title is-5 has-text-weight-normal">评论</h3>
        
<div id="valine-thread" class="content"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='//unpkg.com/valine/dist/Valine.min.js'></script>
<script>
    new Valine({
        el: '#valine-thread' ,
        notify: false,
        verify: false,
        app_id: 'xGYIJxb0wjPnfcBjmscobNQD-gzGzoHsz',
        app_key: 'x45tEyGlPiTsc8rPf8Inee1p',
        placeholder: '风过留痕，君过留言。(欢迎留下昵称和邮箱，以便收到回复通知)'
    });
</script>

    </div>
</div>
</div>
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered" style="flex-shrink: 1">
                <div>
                    
                    <figure class="image is-128x128 has-mb-6">
                        <img class="is-rounded" src="/images/header1.jpg" alt="Roger">
                    </figure>
                    
                    <p class="is-size-4 is-block">
                        Roger
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        好好照顾自己
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>西安</span>
                    </p>
                    
                </div>
            </div>
        </nav>
        <nav class="level is-mobile">
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        文章
                    </p>
                    <a href="/archives">
                        <p class="title has-text-weight-normal">
                            12
                        </p>
                    </a>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        分类
                    </p>
                    <a href="/categories">
                        <p class="title has-text-weight-normal">
                            5
                        </p>
                    </a>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        标签
                    </p>
                    <a href="/tags">
                        <p class="title has-text-weight-normal">
                            10
                        </p>
                    </a>
                </div>
            </div>
        </nav>
        
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://www.zhihu.com/people/zoutq/" target="_blank">
                关注我</a>
        </div>
        
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank"
                title="Github" href="https://github.com/Icecream-blue-sky">
                
                <i class="fab fa-github"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank"
                title="RSS" href="/">
                
                <i class="fas fa-rss"></i>
                
            </a>
            
        </div>
        
       <hr>
       <p id="evan">当我沿着一条路走下去的时候，心里总想着另一条路上的事。这种时候，我心里很乱。放声大哭从一个梦境进入另一个梦境，这是每个人都有的奢望。</p>
    </div>
</div>
    
        

    <div class="card widget" id="toc">
        <div class="card-content">
            <div class="menu">
                <h3 class="menu-label">
                    目录
                </h3>
                <ul class="menu-list"><li>
        <a class="is-flex" href="#1、Adagrad">
        <span class="has-mr-6">1</span>
        <span>1、Adagrad</span>
        </a></li><li>
        <a class="is-flex" href="#2、Stochastic-Gradient-Descent（随机梯度下降法）">
        <span class="has-mr-6">2</span>
        <span>2、Stochastic Gradient Descent（随机梯度下降法）</span>
        </a></li><li>
        <a class="is-flex" href="#3、Momentum（动量法）">
        <span class="has-mr-6">3</span>
        <span>3、Momentum（动量法）</span>
        </a></li><li>
        <a class="is-flex" href="#小结">
        <span class="has-mr-6">4</span>
        <span>小结</span>
        </a></li></ul>
            </div>
        </div>
    </div>

    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
        </div>
    
</div>

                
            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/logo3.jpg" alt="从零开始学习CNN系列——（三）梯度下降法的深度探索" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2021 Icecream&nbsp;
                <a href="https://www.beian.miit.gov.cn" target="_blank">陕ICP备19024510号-1</a>&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a>
                
                <br>
                <span class="busuanzi_container_site_uv">
                
                
                
                <span class="busuanzi_container_site_pv">
                本站总访问量-<span id="busuanzi_value_site_pv"></span>-次 | 您是第-<span id="busuanzi_value_site_uv"></span>-位小伙伴
                </span>
                </span>
                
                </p>
            </div>
            <div class="level-end">
            <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
            <script>
                var now = new Date(); 
                function createtime() { 
                    var grt= new Date("12/8/2019 18:32:00");//此处修改你的建站时间或者网站上线时间 
                    now.setTime(now.getTime()+250); 
                    days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
                    hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
                    if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
                    mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
                    seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
                    snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
                    document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "; 
                    document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
                } 
            setInterval("createtime()",250);
            </script>
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                        
                        <i class="fab fa-creative-commons"></i>&nbsp;<i class="fab fa-creative-commons-by"></i>&nbsp;<i class="fab fa-creative-commons-nc"></i>&nbsp;<i class="fab fa-creative-commons-sa"></i>&nbsp;
                        
                    </a>
                </p>
                
                </div>

            
            </div>
        </div>
    </div>
</footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("zh-CN");</script>

<script>
var IcarusThemeSettings = {
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


    <script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>



    
    
<script src="/js/animation.js"></script>

    
    
<script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
<script src="/js/gallery.js" defer></script>

    
    
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>

    
    <script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>
    
    
<a id="back-to-top" title="回到顶端" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>

    
    
    
    
    
    
    
    
    
    
    
    
    

    


<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="想要查找什么..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":200,"height":400},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>